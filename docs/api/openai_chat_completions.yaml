openapi: 3.0.0
info:
    title: OpenAI API
    description: The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
    version: "2.3.0"
    termsOfService: https://openai.com/policies/terms-of-use
    contact:
        name: OpenAI Support
        url: https://help.openai.com/
    license:
        name: MIT
        url: https://github.com/openai/openai-openapi/blob/master/LICENSE
servers:
    - url: https://api.openai.com/v1
tags:
    - name: Chat
      description: Given a list of messages comprising a conversation, the model will return a response.
    - name: Completions
      description: Given a prompt, the model will return one or more predicted completions, and can also return the probabilities of alternative tokens at each position.
paths:
    # Note: When adding an endpoint, make sure you also add it in the `groups` section, in the end of this file,
    # under the appropriate group
    /chat/completions:
        post:
            operationId: createChatCompletion
            tags:
                - Chat
            summary: Creates a model response for the given chat conversation.
            requestBody:
                required: true
                content:
                    application/json:
                        schema:
                            $ref: "#/components/schemas/CreateChatCompletionRequest"
            responses:
                "200":
                    description: OK
                    content:
                        application/json:
                            schema:
                                $ref: "#/components/schemas/CreateChatCompletionResponse"

            x-oaiMeta:
                name: Create chat completion
                group: chat
                returns: |
                    Returns a [chat completion](/docs/api-reference/chat/object) object, or a streamed sequence of [chat completion chunk](/docs/api-reference/chat/streaming) objects if the request is streamed.
                path: create
                examples:
                    - title: Default
                      request:
                          curl: |
                              curl https://api.openai.com/v1/chat/completions \
                                -H "Content-Type: application/json" \
                                -H "Authorization: Bearer $OPENAI_API_KEY" \
                                -d '{
                                  "model": "VAR_model_id",
                                  "messages": [
                                    {
                                      "role": "system",
                                      "content": "You are a helpful assistant."
                                    },
                                    {
                                      "role": "user",
                                      "content": "Hello!"
                                    }
                                  ]
                                }'
                          python: |
                              from openai import OpenAI
                              client = OpenAI()

                              completion = client.chat.completions.create(
                                model="VAR_model_id",
                                messages=[
                                  {"role": "system", "content": "You are a helpful assistant."},
                                  {"role": "user", "content": "Hello!"}
                                ]
                              )

                              print(completion.choices[0].message)
                          node.js: |-
                              import OpenAI from "openai";

                              const openai = new OpenAI();

                              async function main() {
                                const completion = await openai.chat.completions.create({
                                  messages: [{ role: "system", content: "You are a helpful assistant." }],
                                  model: "VAR_model_id",
                                });

                                console.log(completion.choices[0]);
                              }

                              main();
                      response: &chat_completion_example |
                          {
                            "id": "chatcmpl-123",
                            "object": "chat.completion",
                            "created": 1677652288,
                            "model": "gpt-4o-mini",
                            "system_fingerprint": "fp_44709d6fcb",
                            "choices": [{
                              "index": 0,
                              "message": {
                                "role": "assistant",
                                "content": "\n\nHello there, how may I assist you today?",
                              },
                              "logprobs": null,
                              "finish_reason": "stop"
                            }],
                            "usage": {
                              "prompt_tokens": 9,
                              "completion_tokens": 12,
                              "total_tokens": 21
                            }
                          }
                    - title: Image input
                      request:
                          curl: |
                              curl https://api.openai.com/v1/chat/completions \
                                -H "Content-Type: application/json" \
                                -H "Authorization: Bearer $OPENAI_API_KEY" \
                                -d '{
                                  "model": "gpt-4o",
                                  "messages": [
                                    {
                                      "role": "user",
                                      "content": [
                                        {
                                          "type": "text",
                                          "text": "What'\''s in this image?"
                                        },
                                        {
                                          "type": "image_url",
                                          "image_url": {
                                            "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
                                          }
                                        }
                                      ]
                                    }
                                  ],
                                  "max_tokens": 300
                                }'
                          python: |
                              from openai import OpenAI

                              client = OpenAI()

                              response = client.chat.completions.create(
                                  model="gpt-4o",
                                  messages=[
                                      {
                                          "role": "user",
                                          "content": [
                                              {"type": "text", "text": "What's in this image?"},
                                              {
                                                  "type": "image_url",
                                                  "image_url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
                                              },
                                          ],
                                      }
                                  ],
                                  max_tokens=300,
                              )

                              print(response.choices[0])
                          node.js: |-
                              import OpenAI from "openai";

                              const openai = new OpenAI();

                              async function main() {
                                const response = await openai.chat.completions.create({
                                  model: "gpt-4o",
                                  messages: [
                                    {
                                      role: "user",
                                      content: [
                                        { type: "text", text: "What's in this image?" },
                                        {
                                          type: "image_url",
                                          image_url:
                                            "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
                                        },
                                      ],
                                    },
                                  ],
                                });
                                console.log(response.choices[0]);
                              }
                              main();
                      response: &chat_completion_image_example |
                          {
                            "id": "chatcmpl-123",
                            "object": "chat.completion",
                            "created": 1677652288,
                            "model": "gpt-4o-mini",
                            "system_fingerprint": "fp_44709d6fcb",
                            "choices": [{
                              "index": 0,
                              "message": {
                                "role": "assistant",
                                "content": "\n\nThis image shows a wooden boardwalk extending through a lush green marshland.",
                              },
                              "logprobs": null,
                              "finish_reason": "stop"
                            }],
                            "usage": {
                              "prompt_tokens": 9,
                              "completion_tokens": 12,
                              "total_tokens": 21
                            }
                          }
                    - title: Streaming
                      request:
                          curl: |
                              curl https://api.openai.com/v1/chat/completions \
                                -H "Content-Type: application/json" \
                                -H "Authorization: Bearer $OPENAI_API_KEY" \
                                -d '{
                                  "model": "VAR_model_id",
                                  "messages": [
                                    {
                                      "role": "system",
                                      "content": "You are a helpful assistant."
                                    },
                                    {
                                      "role": "user",
                                      "content": "Hello!"
                                    }
                                  ],
                                  "stream": true
                                }'
                          python: |
                              from openai import OpenAI
                              client = OpenAI()

                              completion = client.chat.completions.create(
                                model="VAR_model_id",
                                messages=[
                                  {"role": "system", "content": "You are a helpful assistant."},
                                  {"role": "user", "content": "Hello!"}
                                ],
                                stream=True
                              )

                              for chunk in completion:
                                print(chunk.choices[0].delta)

                          node.js: |-
                              import OpenAI from "openai";

                              const openai = new OpenAI();

                              async function main() {
                                const completion = await openai.chat.completions.create({
                                  model: "VAR_model_id",
                                  messages: [
                                    {"role": "system", "content": "You are a helpful assistant."},
                                    {"role": "user", "content": "Hello!"}
                                  ],
                                  stream: true,
                                });

                                for await (const chunk of completion) {
                                  console.log(chunk.choices[0].delta.content);
                                }
                              }

                              main();
                      response: &chat_completion_chunk_example |
                          {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini", "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{"role":"assistant","content":""},"logprobs":null,"finish_reason":null}]}

                          {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini", "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{"content":"Hello"},"logprobs":null,"finish_reason":null}]}

                          ....

                          {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini", "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{},"logprobs":null,"finish_reason":"stop"}]}
                    - title: Functions
                      request:
                          curl: |
                              curl https://api.openai.com/v1/chat/completions \
                              -H "Content-Type: application/json" \
                              -H "Authorization: Bearer $OPENAI_API_KEY" \
                              -d '{
                                "model": "gpt-4o",
                                "messages": [
                                  {
                                    "role": "user",
                                    "content": "What'\''s the weather like in Boston today?"
                                  }
                                ],
                                "tools": [
                                  {
                                    "type": "function",
                                    "function": {
                                      "name": "get_current_weather",
                                      "description": "Get the current weather in a given location",
                                      "parameters": {
                                        "type": "object",
                                        "properties": {
                                          "location": {
                                            "type": "string",
                                            "description": "The city and state, e.g. San Francisco, CA"
                                          },
                                          "unit": {
                                            "type": "string",
                                            "enum": ["celsius", "fahrenheit"]
                                          }
                                        },
                                        "required": ["location"]
                                      }
                                    }
                                  }
                                ],
                                "tool_choice": "auto"
                              }'
                          python: |
                              from openai import OpenAI
                              client = OpenAI()

                              tools = [
                                {
                                  "type": "function",
                                  "function": {
                                    "name": "get_current_weather",
                                    "description": "Get the current weather in a given location",
                                    "parameters": {
                                      "type": "object",
                                      "properties": {
                                        "location": {
                                          "type": "string",
                                          "description": "The city and state, e.g. San Francisco, CA",
                                        },
                                        "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                                      },
                                      "required": ["location"],
                                    },
                                  }
                                }
                              ]
                              messages = [{"role": "user", "content": "What's the weather like in Boston today?"}]
                              completion = client.chat.completions.create(
                                model="VAR_model_id",
                                messages=messages,
                                tools=tools,
                                tool_choice="auto"
                              )

                              print(completion)
                          node.js: |-
                              import OpenAI from "openai";

                              const openai = new OpenAI();

                              async function main() {
                                const messages = [{"role": "user", "content": "What's the weather like in Boston today?"}];
                                const tools = [
                                    {
                                      "type": "function",
                                      "function": {
                                        "name": "get_current_weather",
                                        "description": "Get the current weather in a given location",
                                        "parameters": {
                                          "type": "object",
                                          "properties": {
                                            "location": {
                                              "type": "string",
                                              "description": "The city and state, e.g. San Francisco, CA",
                                            },
                                            "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                                          },
                                          "required": ["location"],
                                        },
                                      }
                                    }
                                ];

                                const response = await openai.chat.completions.create({
                                  model: "gpt-4o",
                                  messages: messages,
                                  tools: tools,
                                  tool_choice: "auto",
                                });

                                console.log(response);
                              }

                              main();
                      response: &chat_completion_function_example |
                          {
                            "id": "chatcmpl-abc123",
                            "object": "chat.completion",
                            "created": 1699896916,
                            "model": "gpt-4o-mini",
                            "choices": [
                              {
                                "index": 0,
                                "message": {
                                  "role": "assistant",
                                  "content": null,
                                  "tool_calls": [
                                    {
                                      "id": "call_abc123",
                                      "type": "function",
                                      "function": {
                                        "name": "get_current_weather",
                                        "arguments": "{\n\"location\": \"Boston, MA\"\n}"
                                      }
                                    }
                                  ]
                                },
                                "logprobs": null,
                                "finish_reason": "tool_calls"
                              }
                            ],
                            "usage": {
                              "prompt_tokens": 82,
                              "completion_tokens": 17,
                              "total_tokens": 99
                            }
                          }
                    - title: Logprobs
                      request:
                          curl: |
                              curl https://api.openai.com/v1/chat/completions \
                                -H "Content-Type: application/json" \
                                -H "Authorization: Bearer $OPENAI_API_KEY" \
                                -d '{
                                  "model": "VAR_model_id",
                                  "messages": [
                                    {
                                      "role": "user",
                                      "content": "Hello!"
                                    }
                                  ],
                                  "logprobs": true,
                                  "top_logprobs": 2
                                }'
                          python: |
                              from openai import OpenAI
                              client = OpenAI()

                              completion = client.chat.completions.create(
                                model="VAR_model_id",
                                messages=[
                                  {"role": "user", "content": "Hello!"}
                                ],
                                logprobs=True,
                                top_logprobs=2
                              )

                              print(completion.choices[0].message)
                              print(completion.choices[0].logprobs)
                          node.js: |-
                              import OpenAI from "openai";

                              const openai = new OpenAI();

                              async function main() {
                                const completion = await openai.chat.completions.create({
                                  messages: [{ role: "user", content: "Hello!" }],
                                  model: "VAR_model_id",
                                  logprobs: true,
                                  top_logprobs: 2,
                                });

                                console.log(completion.choices[0]);
                              }

                              main();
                      response: |
                          {
                            "id": "chatcmpl-123",
                            "object": "chat.completion",
                            "created": 1702685778,
                            "model": "gpt-4o-mini",
                            "choices": [
                              {
                                "index": 0,
                                "message": {
                                  "role": "assistant",
                                  "content": "Hello! How can I assist you today?"
                                },
                                "logprobs": {
                                  "content": [
                                    {
                                      "token": "Hello",
                                      "logprob": -0.31725305,
                                      "bytes": [72, 101, 108, 108, 111],
                                      "top_logprobs": [
                                        {
                                          "token": "Hello",
                                          "logprob": -0.31725305,
                                          "bytes": [72, 101, 108, 108, 111]
                                        },
                                        {
                                          "token": "Hi",
                                          "logprob": -1.3190403,
                                          "bytes": [72, 105]
                                        }
                                      ]
                                    },
                                    {
                                      "token": "!",
                                      "logprob": -0.02380986,
                                      "bytes": [
                                        33
                                      ],
                                      "top_logprobs": [
                                        {
                                          "token": "!",
                                          "logprob": -0.02380986,
                                          "bytes": [33]
                                        },
                                        {
                                          "token": " there",
                                          "logprob": -3.787621,
                                          "bytes": [32, 116, 104, 101, 114, 101]
                                        }
                                      ]
                                    },
                                    {
                                      "token": " How",
                                      "logprob": -0.000054669687,
                                      "bytes": [32, 72, 111, 119],
                                      "top_logprobs": [
                                        {
                                          "token": " How",
                                          "logprob": -0.000054669687,
                                          "bytes": [32, 72, 111, 119]
                                        },
                                        {
                                          "token": "<|end|>",
                                          "logprob": -10.953937,
                                          "bytes": null
                                        }
                                      ]
                                    },
                                    {
                                      "token": " can",
                                      "logprob": -0.015801601,
                                      "bytes": [32, 99, 97, 110],
                                      "top_logprobs": [
                                        {
                                          "token": " can",
                                          "logprob": -0.015801601,
                                          "bytes": [32, 99, 97, 110]
                                        },
                                        {
                                          "token": " may",
                                          "logprob": -4.161023,
                                          "bytes": [32, 109, 97, 121]
                                        }
                                      ]
                                    },
                                    {
                                      "token": " I",
                                      "logprob": -3.7697225e-6,
                                      "bytes": [
                                        32,
                                        73
                                      ],
                                      "top_logprobs": [
                                        {
                                          "token": " I",
                                          "logprob": -3.7697225e-6,
                                          "bytes": [32, 73]
                                        },
                                        {
                                          "token": " assist",
                                          "logprob": -13.596657,
                                          "bytes": [32, 97, 115, 115, 105, 115, 116]
                                        }
                                      ]
                                    },
                                    {
                                      "token": " assist",
                                      "logprob": -0.04571125,
                                      "bytes": [32, 97, 115, 115, 105, 115, 116],
                                      "top_logprobs": [
                                        {
                                          "token": " assist",
                                          "logprob": -0.04571125,
                                          "bytes": [32, 97, 115, 115, 105, 115, 116]
                                        },
                                        {
                                          "token": " help",
                                          "logprob": -3.1089056,
                                          "bytes": [32, 104, 101, 108, 112]
                                        }
                                      ]
                                    },
                                    {
                                      "token": " you",
                                      "logprob": -5.4385737e-6,
                                      "bytes": [32, 121, 111, 117],
                                      "top_logprobs": [
                                        {
                                          "token": " you",
                                          "logprob": -5.4385737e-6,
                                          "bytes": [32, 121, 111, 117]
                                        },
                                        {
                                          "token": " today",
                                          "logprob": -12.807695,
                                          "bytes": [32, 116, 111, 100, 97, 121]
                                        }
                                      ]
                                    },
                                    {
                                      "token": " today",
                                      "logprob": -0.0040071653,
                                      "bytes": [32, 116, 111, 100, 97, 121],
                                      "top_logprobs": [
                                        {
                                          "token": " today",
                                          "logprob": -0.0040071653,
                                          "bytes": [32, 116, 111, 100, 97, 121]
                                        },
                                        {
                                          "token": "?",
                                          "logprob": -5.5247097,
                                          "bytes": [63]
                                        }
                                      ]
                                    },
                                    {
                                      "token": "?",
                                      "logprob": -0.0008108172,
                                      "bytes": [63],
                                      "top_logprobs": [
                                        {
                                          "token": "?",
                                          "logprob": -0.0008108172,
                                          "bytes": [63]
                                        },
                                        {
                                          "token": "?\n",
                                          "logprob": -7.184561,
                                          "bytes": [63, 10]
                                        }
                                      ]
                                    }
                                  ]
                                },
                                "finish_reason": "stop"
                              }
                            ],
                            "usage": {
                              "prompt_tokens": 9,
                              "completion_tokens": 9,
                              "total_tokens": 18
                            },
                            "system_fingerprint": null
                          }

components:
    
    schemas:
        
        CreateChatCompletionRequest:
            type: object
            properties:
                messages:
                    description: A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).
                    type: array
                    minItems: 1
                    items:
                        $ref: "https://raw.githubusercontent.com/openai/openai-openapi/master/openapi.yaml#/components/schemas/ChatCompletionRequestMessage"
                model:
                    description: ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API.
                    example: "gpt-4o"
                    anyOf:
                        - type: string
                        - type: string
                          enum:
                              [
                                  "gpt-4o",
                                  "gpt-4o-2024-05-13",
                                  "gpt-4o-2024-08-06",
                                  "chatgpt-4o-latest",
                                  "gpt-4o-mini",
                                  "gpt-4o-mini-2024-07-18",
                                  "gpt-4-turbo",
                                  "gpt-4-turbo-2024-04-09",
                                  "gpt-4-0125-preview",
                                  "gpt-4-turbo-preview",
                                  "gpt-4-1106-preview",
                                  "gpt-4-vision-preview",
                                  "gpt-4",
                                  "gpt-4-0314",
                                  "gpt-4-0613",
                                  "gpt-4-32k",
                                  "gpt-4-32k-0314",
                                  "gpt-4-32k-0613",
                                  "gpt-3.5-turbo",
                                  "gpt-3.5-turbo-16k",
                                  "gpt-3.5-turbo-0301",
                                  "gpt-3.5-turbo-0613",
                                  "gpt-3.5-turbo-1106",
                                  "gpt-3.5-turbo-0125",
                                  "gpt-3.5-turbo-16k-0613",
                              ]
                    x-oaiTypeLabel: string
                frequency_penalty:
                    type: number
                    default: 0
                    minimum: -2
                    maximum: 2
                    nullable: true
                    description: &completions_frequency_penalty_description |
                        Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.

                        [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)
                logit_bias:
                    type: object
                    x-oaiTypeLabel: map
                    default: null
                    nullable: true
                    additionalProperties:
                        type: integer
                    description: |
                        Modify the likelihood of specified tokens appearing in the completion.

                        Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
                logprobs:
                    description: Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.
                    type: boolean
                    default: false
                    nullable: true
                top_logprobs:
                    description: An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.
                    type: integer
                    minimum: 0
                    maximum: 20
                    nullable: true
                max_tokens:
                    description: |
                        The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.

                        The total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.
                    type: integer
                    nullable: true
                n:
                    type: integer
                    minimum: 1
                    maximum: 128
                    default: 1
                    example: 1
                    nullable: true
                    description: How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.
                presence_penalty:
                    type: number
                    default: 0
                    minimum: -2
                    maximum: 2
                    nullable: true
                    description: &completions_logit_bias_description |
                        Modify the likelihood of specified tokens appearing in the completion.

                        Accepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this [tokenizer tool](/tokenizer?view=bpe) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.

                        As an example, you can pass `{"50256": -100}` to prevent the <|endoftext|> token from being generated.
                response_format:
                    description: |
                        An object specifying the format that the model must output. Compatible with [GPT-4o](/docs/models/gpt-4o), [GPT-4o mini](/docs/models/gpt-4o-mini), [GPT-4 Turbo](/docs/models/gpt-4-and-gpt-4-turbo) and all GPT-3.5 Turbo models newer than `gpt-3.5-turbo-1106`.

                        Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured Outputs which guarantees the model will match your supplied JSON schema. Learn more in the [Structured Outputs guide](/docs/guides/structured-outputs).

                        Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is valid JSON.

                        **Important:** when using JSON mode, you **must** also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly "stuck" request. Also note that the message content may be partially cut off if `finish_reason="length"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length.
                    oneOf:
                      - $ref: "https://raw.githubusercontent.com/openai/openai-openapi/master/openapi.yaml#/components/schemas/ResponseFormatText"
                      - $ref: "https://raw.githubusercontent.com/openai/openai-openapi/master/openapi.yaml#/components/schemas/ResponseFormatJsonObject"
                      - $ref: "https://raw.githubusercontent.com/openai/openai-openapi/master/openapi.yaml#/components/schemas/ResponseFormatJsonSchema"
                    x-oaiExpandable: true
                seed:
                    type: integer
                    minimum: -9223372036854775808
                    maximum: 9223372036854775807
                    nullable: true
                    description: |
                        This feature is in Beta.
                        If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.
                        Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
                    x-oaiMeta:
                        beta: true
                service_tier:
                  description: |
                      Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:
                        - If set to 'auto', the system will utilize scale tier credits until they are exhausted.
                        - If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.
                        - When not set, the default behavior is 'auto'.

                        When this parameter is set, the response body will include the `service_tier` utilized.
                  type: string
                  enum: ["auto", "default"]
                  nullable: true
                stop:
                    description: |
                        Up to 4 sequences where the API will stop generating further tokens.
                    default: null
                    oneOf:
                        - type: string
                          nullable: true
                        - type: array
                          minItems: 1
                          maxItems: 4
                          items:
                              type: string
                stream:
                    description: >
                        If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
                        as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).
                    type: boolean
                    nullable: true
                    default: false
                stream_options:
                    $ref: "https://raw.githubusercontent.com/openai/openai-openapi/master/openapi.yaml#/components/schemas/ChatCompletionStreamOptions"
                temperature:
                    type: number
                    minimum: 0
                    maximum: 2
                    default: 1
                    example: 1
                    nullable: true
                    description: &completions_temperature_description |
                        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.

                        We generally recommend altering this or `top_p` but not both.
                top_p:
                    type: number
                    minimum: 0
                    maximum: 1
                    default: 1
                    example: 1
                    nullable: true
                    description: &completions_top_p_description |
                        An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.

                        We generally recommend altering this or `temperature` but not both.
                tools:
                    type: array
                    description: >
                        A list of tools the model may call. Currently, only functions are supported as a tool.
                        Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.
                    items:
                        $ref: "https://raw.githubusercontent.com/openai/openai-openapi/master/openapi.yaml#/components/schemas/ChatCompletionTool"
                tool_choice:
                    $ref: "https://raw.githubusercontent.com/openai/openai-openapi/master/openapi.yaml#/components/schemas/ChatCompletionToolChoiceOption"
                parallel_tool_calls:
                    $ref: "https://raw.githubusercontent.com/openai/openai-openapi/master/openapi.yaml#/components/schemas/ParallelToolCalls"
                user: &end_user_param_configuration
                    type: string
                    example: user-1234
                    description: |
                        A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
                function_call:
                    deprecated: true
                    description: |
                        Deprecated in favor of `tool_choice`.

                        Controls which (if any) function is called by the model.
                        `none` means the model will not call a function and instead generates a message.
                        `auto` means the model can pick between generating a message or calling a function.
                        Specifying a particular function via `{"name": "my_function"}` forces the model to call that function.

                        `none` is the default when no functions are present. `auto` is the default if functions are present.
                    oneOf:
                        - type: string
                          description: >
                              `none` means the model will not call a function and instead generates a message.
                              `auto` means the model can pick between generating a message or calling a function.
                          enum: [none, auto]
                        - $ref: "https://raw.githubusercontent.com/openai/openai-openapi/master/openapi.yaml#/components/schemas/ChatCompletionFunctionCallOption"
                    x-oaiExpandable: true
                functions:
                    deprecated: true
                    description: |
                        Deprecated in favor of `tools`.

                        A list of functions the model may generate JSON inputs for.
                    type: array
                    minItems: 1
                    maxItems: 128
                    items:
                        $ref: "https://raw.githubusercontent.com/openai/openai-openapi/master/openapi.yaml#/components/schemas/ChatCompletionFunctions"

                detectors:
                  $ref: "https://raw.githubusercontent.com/evaline-ju/fms-guardrails-orchestrator/refs/heads/chat-detections-api/docs/api/orchestrator_openapi_0_1_0.yaml#/components/schemas/Detectors"
                  # type: object
                  # title: Detectors
                  # properties:
                  #   input:
                  #     type: object
                  #     title: Input Detectors
                  #     default: {}
                  #   output:
                  #     type: object
                  #     title: Output Detectors
                  #     default: {}
                  # default: {}
                  # example:
                  #   input:
                  #     hap-v1-model-en: {}
                  #   output:
                  #     pii-v1: {}
                  #     egregious_conversation: {}
            required:
                - model
                - messages

        CreateChatCompletionResponse:
            type: object
            description: Represents a chat completion response returned by model, based on the provided input.
            properties:
                id:
                    type: string
                    description: A unique identifier for the chat completion.
                choices:
                    type: array
                    description: A list of chat completion choices. Can be more than one if `n` is greater than 1.
                    items:
                        type: object
                        required:
                            - finish_reason
                            - index
                            - message
                            - logprobs
                        properties:
                            finish_reason:
                                type: string
                                description: &chat_completion_finish_reason_description |
                                    The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,
                                    `length` if the maximum number of tokens specified in the request was reached,
                                    `content_filter` if content was omitted due to a flag from our content filters,
                                    `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function.
                                enum:
                                    [
                                        "stop",
                                        "length",
                                        "tool_calls",
                                        "content_filter",
                                        "function_call",
                                    ]
                            index:
                                type: integer
                                description: The index of the choice in the list of choices.
                            message:
                                $ref: "https://raw.githubusercontent.com/openai/openai-openapi/master/openapi.yaml#/components/schemas/ChatCompletionResponseMessage"
                            logprobs: &chat_completion_response_logprobs
                                description: Log probability information for the choice.
                                type: object
                                nullable: true
                                properties:
                                    content:
                                        description: A list of message content tokens with log probability information.
                                        type: array
                                        items:
                                            $ref: "https://raw.githubusercontent.com/openai/openai-openapi/master/openapi.yaml#/components/schemas/ChatCompletionTokenLogprob"
                                        nullable: true
                                    refusal:
                                        description: A list of message refusal tokens with log probability information.
                                        type: array
                                        items:
                                            $ref: "https://raw.githubusercontent.com/openai/openai-openapi/master/openapi.yaml#/components/schemas/ChatCompletionTokenLogprob"
                                        nullable: true
                                required:
                                    - content
                                    - refusal

                created:
                    type: integer
                    description: The Unix timestamp (in seconds) of when the chat completion was created.
                model:
                    type: string
                    description: The model used for the chat completion.
                service_tier:
                    description: The service tier used for processing the request. This field is only included if the `service_tier` parameter is specified in the request.
                    type: string
                    enum: ["scale", "default"]
                    example: "scale"
                    nullable: true
                system_fingerprint:
                    type: string
                    description: |
                        This fingerprint represents the backend configuration that the model runs with.

                        Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
                object:
                    type: string
                    description: The object type, which is always `chat.completion`.
                    enum: [chat.completion]
                usage:
                    $ref: "https://raw.githubusercontent.com/openai/openai-openapi/master/openapi.yaml#/components/schemas/CompletionUsage"
                  
                detections:
                  type: object
                  description: Detections from guardrails framework
                  example:
                    input:
                      {}
                    output:
                      - {
                        "choice": 1,
                        "results": [
                          {
                            "start": 0,
                            "end": 20,
                            "text": "string",
                            "detection_type": "HAP",
                            "detection": "has_HAP",
                            "detector_id": "hap-v1-model-en",
                            "score": 0.999
                          },
                          {
                            "detection_type": "string",
                            "detection": "string",
                            "detector_id": "relevance-v1-en",
                            "score": 0
                          }
                        ]
                      }
                      - {
                        "choice": 2
                      }
                      - {
                        "choice": "all",
                        "results": [
                          {
                            "detection_type": "string",
                            "detection": "string",
                            "detector_id": "unknown-v1-en",
                            "score": 0
                          }
                        ]
                      }
                
                warnings:
                  type: object
                  example:
                    - type: incompatible-generation
                      message: detector X could not run because of no content in choice 1
            required:
                - choices
                - created
                - id
                - model
                - object
            x-oaiMeta:
                name: The chat completion object
                group: chat
                example: *chat_completion_example
        
        CreateChatCompletionStreamResponse:
            type: object
            description: Represents a streamed chunk of a chat completion response returned by model, based on the provided input.
            properties:
                id:
                    type: string
                    description: A unique identifier for the chat completion. Each chunk has the same ID.
                choices:
                    type: array
                    description: |
                        A list of chat completion choices. Can contain more than one elements if `n` is greater than 1. Can also be empty for the
                        last chunk if you set `stream_options: {"include_usage": true}`.
                    items:
                        type: object
                        required:
                            - delta
                            - finish_reason
                            - index
                        properties:
                            delta:
                                $ref: "https://raw.githubusercontent.com/openai/openai-openapi/master/openapi.yaml#/components/schemas/ChatCompletionStreamResponseDelta"
                            logprobs: *chat_completion_response_logprobs
                            finish_reason:
                                type: string
                                description: *chat_completion_finish_reason_description
                                enum:
                                    [
                                        "stop",
                                        "length",
                                        "tool_calls",
                                        "content_filter",
                                        "function_call",
                                    ]
                                nullable: true
                            index:
                                type: integer
                                description: The index of the choice in the list of choices.
                created:
                    type: integer
                    description: The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.
                model:
                    type: string
                    description: The model to generate the completion.
                service_tier:
                    description: The service tier used for processing the request. This field is only included if the `service_tier` parameter is specified in the request.
                    type: string
                    enum: ["scale", "default"]
                    example: "scale"
                    nullable: true
                system_fingerprint:
                    type: string
                    description: |
                        This fingerprint represents the backend configuration that the model runs with.
                        Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
                object:
                    type: string
                    description: The object type, which is always `chat.completion.chunk`.
                    enum: [chat.completion.chunk]
                usage:
                    type: object
                    description: |
                        An optional field that will only be present when you set `stream_options: {"include_usage": true}` in your request.
                        When present, it contains a null value except for the last chunk which contains the token usage statistics for the entire request.
                    properties:
                        completion_tokens:
                            type: integer
                            description: Number of tokens in the generated completion.
                        prompt_tokens:
                            type: integer
                            description: Number of tokens in the prompt.
                        total_tokens:
                            type: integer
                            description: Total number of tokens used in the request (prompt + completion).
                    required:
                        - prompt_tokens
                        - completion_tokens
                        - total_tokens
            required:
                - choices
                - created
                - id
                - model
                - object
            x-oaiMeta:
                name: The chat completion chunk object
                group: chat
                example: *chat_completion_chunk_example
